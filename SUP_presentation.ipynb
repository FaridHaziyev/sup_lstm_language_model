{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is Language Modeling ?\n",
    "- Statistical Language Models\n",
    "- NGRAMS\n",
    "- Neural Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Language Modeling ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deterimining the probability of seeing a group of words together in a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application Areas\n",
    "\n",
    "- Machine Translation\n",
    "    - P(high winds tonite) > P(large winds tonite)\n",
    "- Speech Recognition\n",
    "    - P(I saw a van) >> P(eyes awe of an) \n",
    "- Spell Correction\n",
    "    - The office is about fifteen minuets from my house \n",
    "    - P(about fifteen minutes from) > P(about fifteen minuets from)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Autocomplete\n",
    "\n",
    "\n",
    "![title](https://i.chzbgr.com/full/5734002944/h1074620A/google-autocomplete-fail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(\"bugün hava soyuq olacaq\") = P(\"bugün\") * P(\"hava\" | \"bugün\") * P(\"soyuq\" | \"bugün\", \"hava\") * P(\"olacaq\" | \"bugün\", \"hava\", \"soyuq\")\n",
    "\n",
    "- Difficult to calculate as the sentence could be very long\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look at only the last n words when predicting the current word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unigram:** \n",
    "    - P(\"bugün hava soyuq olacaq\") = P(\"bugün\") * P(\"hava\") * P(\"soyuq\") * P(\"olacaq\")\n",
    "**Bigram:**\n",
    "    - P(\"bugün hava soyuq olacaq\") = P(\"bugün\") * P(\"hava\" | \"bugün\") * P(\"soyuq\" | \"hava\") * P(\"olacaq\" | \"soyuq\")\n",
    "**Trigram:**\n",
    "    - P(\"bugün hava soyuq olacaq\") = P(\"bugün\") * P(\"hava\" | \"bugün\") * P(\"soyuq\" | \"bugün\", \"hava\") * P(\"olacaq\" | \"hava\", \"soyuq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Sentence Generator using bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "import pickle, re, pdb\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bigrams\n",
    "probabilities = dict()\n",
    "\n",
    "seeds = [\"sən\",\"mən\", \"avarə\", \"camahat\", \"pulsuz\", \"kasıb\", \"oxumaq\", \"fəhlə\", \"kişi\", \"qadın\", \"fələk\", \"ey\", \"heyhat\"]\n",
    "\n",
    "\n",
    "with open(\"sabir.txt\", \"r\", encoding=\"utf8\") as fr:\n",
    "    for line in fr:\n",
    "        words = re.findall(\"\\w+\", line.strip().replace(\"İ\", \"i\").lower())\n",
    "        for i,word in enumerate(words):            \n",
    "            if i < len(words) - 1:\n",
    "                if word not in probabilities:\n",
    "                    probabilities[word] = dict()\n",
    "\n",
    "                probabilities[word][words[i+1]] = probabilities[word].get(words[i+1],0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_bigram(seed_words, num_words):\n",
    "    texts = []\n",
    "    for seed in seed_words:\n",
    "        text = seed\n",
    "        for i in range(num_words):\n",
    "            if seed not in probabilities:\n",
    "                continue\n",
    "            pr_word = max(probabilities[seed].items(), key=operator.itemgetter(1))[0]\n",
    "            text += \" \" + pr_word\n",
    "            seed = pr_word\n",
    "        \n",
    "        texts.append(text)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sən də özün daxili insan edir',\n",
       " 'mən kimi bir də özün daxili',\n",
       " 'avarə səbr eylə qeybətdə rübərüdə',\n",
       " 'camahat',\n",
       " 'pulsuz kişi insanlığı asanmı sanırsan sənin',\n",
       " 'kasıb',\n",
       " 'oxumaq suglu kitab açdır fala bax',\n",
       " 'fəhlə də özün daxili insan edir',\n",
       " 'kişi insanlığı asanmı sanırsan sənin ancaq',\n",
       " 'qadın',\n",
       " 'fələk tərsinə dövran edir imdi duayə',\n",
       " 'ey əmu',\n",
       " 'heyhat ki razi pünhan xahəd şod']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_bigram(seeds, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "with open(\"sabir.txt\", \"r\", encoding=\"utf8\") as fr:\n",
    "    for line in fr:\n",
    "        line = line.strip().replace(\"İ\", \"i\").lower()\n",
    "        sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "index_to_word = {id_:word for word, id_ in tokenizer.word_index.items()}\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=3)\n",
    "\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "tokenizer.num_words = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating input data\n",
    "\n",
    "input_sequences = []\n",
    "\n",
    "for line in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0] #converts words in sentences to ids\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max([len(sentence) for sentence in input_sequences])\n",
    "\n",
    "input_sequences = sequence.pad_sequences(input_sequences,\n",
    "                                         maxlen=MAX_LEN,\n",
    "                                         padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = input_sequences[:, :-1]\n",
    "labels = input_sequences[:, -1]\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hektor/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/hektor/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/hektor/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/hektor/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/hektor/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/hektor/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "11220/11220 [==============================] - 18s 2ms/sample - loss: 8.3408 - acc: 0.0157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe92ac301d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=MAX_LEN-1))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "\n",
    "model.add(Dense(total_words, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(xs, ys, epochs = 1, verbose=1) #epoch sayın artıraraq daha çox train ede bilirik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the trained model\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use your model to generate sentences\n",
    "\n",
    "def generate_with_lstm(tokenizer, seeds, num_words, max_len):\n",
    "    generated_texts = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        for _ in range(num_words):\n",
    "            token_list = tokenizer.texts_to_sequences([seed])[0]\n",
    "            token_list = sequence.pad_sequences([token_list], maxlen=max_len-1, padding=\"pre\")\n",
    "\n",
    "            predicted = model.predict_classes(token_list, verbose=0)\n",
    "            output_word = index_to_word[predicted[0]]\n",
    "\n",
    "            if output_word != seed.split(\" \")[-1]:\n",
    "                seed +=  \" \" + output_word\n",
    "        generated_texts.append(seed)\n",
    "\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sən nə',\n",
       " 'mən bir',\n",
       " 'avarə bir',\n",
       " 'camahat bir',\n",
       " 'pulsuz bir',\n",
       " 'kasıb bir',\n",
       " 'oxumaq bir',\n",
       " 'fəhlə nə',\n",
       " 'kişi nə',\n",
       " 'qadın bir',\n",
       " 'fələk nə',\n",
       " 'ey bir',\n",
       " 'heyhat bir']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_lstm(tokenizer, seeds, 5, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sən nə',\n",
       " 'mən bir',\n",
       " 'avarə bir',\n",
       " 'camahat bir',\n",
       " 'pulsuz bir',\n",
       " 'kasıb bir',\n",
       " 'oxumaq bir',\n",
       " 'fəhlə nə',\n",
       " 'kişi nə',\n",
       " 'qadın bir',\n",
       " 'fələk nə',\n",
       " 'ey bir',\n",
       " 'heyhat bir']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If you want to load your already trained model and use\n",
    "def load_model(tokenizer, model_js, model_w):\n",
    "    with open(tokenizer, \"rb\") as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    json_file = open(model_js, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "\n",
    "    model.load_weights(model_w)\n",
    "    index_to_word = {id_:word for word, id_ in tokenizer.word_index.items()}\n",
    "    \n",
    "    return tokenizer, model, index_to_word\n",
    "\n",
    "tokenizer, model, index_to_word = load_model(\"tokenizer.pickle\", \"model.json\", \"model.h5\")\n",
    "generate_with_lstm(tokenizer, seeds, 5, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
